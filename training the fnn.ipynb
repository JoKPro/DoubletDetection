{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preparing for doublet.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "HyA-XjSe9-M3",
        "lyM7s7W48Zf_",
        "WjtU_WCB86_d"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import the data\n",
        "We want the doubletannotation.csv as the label file.\n",
        "The pbmc_expr.csv can be the x \n",
        "doubletannotation.csv can be the y"
      ],
      "metadata": {
        "id": "HyA-XjSe9-M3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install 'fsspec>=0.3.3'\n",
        "!python -m pip install dask[dataframe] --upgrade  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_ibnqzVEhBb",
        "outputId": "3815a3dd-ebab-487e-e362-e7a2d5108e36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fsspec>=0.3.3\n",
            "  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 4.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: fsspec\n",
            "Successfully installed fsspec-2022.5.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: dask[dataframe] in /usr/local/lib/python3.7/dist-packages (2.12.0)\n",
            "Requirement already satisfied: toolz>=0.7.3 in /usr/local/lib/python3.7/dist-packages (from dask[dataframe]) (0.12.0)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from dask[dataframe]) (2022.5.0)\n",
            "Collecting partd>=0.3.10\n",
            "  Downloading partd-1.2.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: numpy>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from dask[dataframe]) (1.21.6)\n",
            "Requirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from dask[dataframe]) (1.3.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.0->dask[dataframe]) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.0->dask[dataframe]) (2.8.2)\n",
            "Collecting locket\n",
            "  Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.23.0->dask[dataframe]) (1.15.0)\n",
            "Installing collected packages: locket, partd\n",
            "Successfully installed locket-1.0.0 partd-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "test_data = 0\n",
        "train_data = 0\n",
        "val_data = 0\n"
      ],
      "metadata": {
        "id": "JZdBbuAHBWEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import required modules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from dask import dataframe as df1\n",
        "  \n",
        "# time taken to read data\n",
        "s_time_dask = time.time()\n",
        "dask_df = df1.read_csv('pbmc_expr.csv', assume_missing=True)\n",
        "e_time_dask = time.time()\n",
        "  \n",
        "print(\"Read with dask: \", (e_time_dask-s_time_dask), \"seconds\")\n",
        "  \n",
        "# data\n",
        "dask_df.head(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "LQVe8B7ZEUaZ",
        "outputId": "13dd4a4a-ceb4-49d4-d195-4aec8ff53fbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read with dask:  8.11897873878479 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Unnamed: 0  AAACAGCCAAAGCCTC.1  AAACAGCCAGAATGAC.1  AAACAGCCAGCTACGT.1  \\\n",
              "0  MIR1302-2HG                 0.0                 0.0                 0.0   \n",
              "\n",
              "   AAACAGCCAGGCCTTG.1  AAACAGCCATAAGGAC.1  AAACAGCCATAATGTC.1  \\\n",
              "0                 0.0                 0.0                 0.0   \n",
              "\n",
              "   AAACATGCAGCAATAA.1  AAACATGCAGCCAGAA.1  AAACATGCAGGCGATA.1  ...  \\\n",
              "0                 0.0                 0.0                 0.0  ...   \n",
              "\n",
              "   TTTGTGGCATGAATCT.1  TTTGTGTTCAGTTCCC.1  TTTGTGTTCCCTCATA.1  \\\n",
              "0                 0.0                 0.0                 0.0   \n",
              "\n",
              "   TTTGTGTTCCTTGAGG.1  TTTGTGTTCTAGCTAA.1  TTTGTTGGTACGCGCA.1  \\\n",
              "0                 0.0                 0.0                 0.0   \n",
              "\n",
              "   TTTGTTGGTATTTGCC.1  TTTGTTGGTGATTACG.1  TTTGTTGGTTTCAGGA.1  \\\n",
              "0                 0.0                 0.0                 0.0   \n",
              "\n",
              "   TTTGTTGGTTTCCACG.1  \n",
              "0                 0.0  \n",
              "\n",
              "[1 rows x 12013 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-573152d0-9825-4069-b76d-827712404c54\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>AAACAGCCAAAGCCTC.1</th>\n",
              "      <th>AAACAGCCAGAATGAC.1</th>\n",
              "      <th>AAACAGCCAGCTACGT.1</th>\n",
              "      <th>AAACAGCCAGGCCTTG.1</th>\n",
              "      <th>AAACAGCCATAAGGAC.1</th>\n",
              "      <th>AAACAGCCATAATGTC.1</th>\n",
              "      <th>AAACATGCAGCAATAA.1</th>\n",
              "      <th>AAACATGCAGCCAGAA.1</th>\n",
              "      <th>AAACATGCAGGCGATA.1</th>\n",
              "      <th>...</th>\n",
              "      <th>TTTGTGGCATGAATCT.1</th>\n",
              "      <th>TTTGTGTTCAGTTCCC.1</th>\n",
              "      <th>TTTGTGTTCCCTCATA.1</th>\n",
              "      <th>TTTGTGTTCCTTGAGG.1</th>\n",
              "      <th>TTTGTGTTCTAGCTAA.1</th>\n",
              "      <th>TTTGTTGGTACGCGCA.1</th>\n",
              "      <th>TTTGTTGGTATTTGCC.1</th>\n",
              "      <th>TTTGTTGGTGATTACG.1</th>\n",
              "      <th>TTTGTTGGTTTCAGGA.1</th>\n",
              "      <th>TTTGTTGGTTTCCACG.1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>MIR1302-2HG</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 12013 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-573152d0-9825-4069-b76d-827712404c54')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-573152d0-9825-4069-b76d-827712404c54 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-573152d0-9825-4069-b76d-827712404c54');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dialect = 0\n",
        "with open('pbmc_expr.csv', newline='') as csvfile:\n",
        "    dialect = csv.Sniffer().sniff(csvfile.read(1024))\n",
        "    csvfile.seek(0)\n",
        "    print(csvfile.seek(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arTvMgtzBU01",
        "outputId": "64607e23-3cab-4924-fd0f-8ede9eb35580"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# import x\n",
        "with open('pbmc_expr.csv', newline='') as csvfile:\n",
        "    reader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
        "    reader\n",
        "    for row in reader:\n",
        "        print(', '.join(row))"
      ],
      "metadata": {
        "id": "Gn9LXAiRCSLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eigener Dataloader 4 size"
      ],
      "metadata": {
        "id": "-uCWK1K2WXua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision import transforms\n",
        "\n",
        "class CustomDataloaderold(Dataset):\n",
        "    def __init__(self, dast):\n",
        "        self.dast = dast\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.dast.index.stop\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        entry = self.dast.iloc[idx]\n",
        "        if type(idx) == slice:\n",
        "          xent = np.vstack(entry.values[:,2:6]).astype(np.float)\n",
        "          x = torch.from_numpy(xent).float()\n",
        "          yent = np.vstack(entry.values[:,6:]).astype(np.float)\n",
        "          y = torch.from_numpy(yent).float()\n",
        "        else:\n",
        "          x = torch.tensor(entry[2:6]).float()\n",
        "          y = torch.tensor([float(int(i)) for i in entry[6:]]).float()\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "zXK8-FfVWXHO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eigener Dataloader 103 size"
      ],
      "metadata": {
        "id": "-38aiY4aPjsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision import transforms\n",
        "\n",
        "class CustomDataloader(Dataset):\n",
        "    def __init__(self, dast):\n",
        "        self.dast = dast\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.dast.index.stop\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        entry = self.dast.iloc[idx]\n",
        "        if type(idx) == slice:\n",
        "          xent = np.vstack(entry.values[:,:-3]).astype(np.float)\n",
        "          x = torch.from_numpy(xent).float()\n",
        "          yent = np.vstack(entry.values[:,-3:]).astype(np.float)\n",
        "          y = torch.from_numpy(yent).float()\n",
        "        else:\n",
        "          x = torch.tensor(entry[:-3]).float()\n",
        "          y = torch.tensor([float(int(i)) for i in entry[-3:]]).float()\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "WoZmwe8lPm1c"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading data from csv"
      ],
      "metadata": {
        "id": "SQObqJOJ8zhe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "w9t89EATkcXW",
        "outputId": "67e73f74-b708-4379-bd4f-98ba1f086dff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "(362, 9)\n",
            "(290, 9)\n",
            "(300, 9)\n",
            "[[4865 'CGTCATTGTAGGTGTC.1' 0.773191750049591 ... True False False]\n",
            " [6858 'GCCGTGAGTGGTTCCC.1' 0.0421629399061203 ... False False True]\n",
            " [6681 'GCACTAAGTGGATTAT.1' 0.732089161872864 ... False True False]\n",
            " ...\n",
            " [1556 'AGCAACAAGGGTGAAC.1' 0.890693664550781 ... False True False]\n",
            " [2038 'AGTACGCGTTTCCGGC.1' 0.954596340656281 ... True False False]\n",
            " [4684 'CGGAATCGTTCCGGGA.1' 0.732701778411865 ... True False False]]\n",
            "     idxs                  V1    bcds_s          cxds_s   dbf_s     chord  \\\n",
            "0    4865  CGTCATTGTAGGTGTC.1  0.773192   806126.806729  0.6125 -1.196678   \n",
            "1    6858  GCCGTGAGTGGTTCCC.1  0.042163   207762.742497   0.025 -1.992952   \n",
            "2    6681  GCACTAAGTGGATTAT.1  0.732089   279032.772917   0.175 -2.065139   \n",
            "3    6855  GCCGTGAGTCAGTAAT.1  0.915708  1151176.904928  0.6125 -1.316453   \n",
            "4    4976  CTAAAGCTCATAGCCG.1  0.020389   127844.357496   0.025 -1.994257   \n",
            "..    ...                 ...       ...             ...     ...       ...   \n",
            "947  1829  AGGATCCGTGGAGCAA.1  0.745182   420345.627035  0.5625 -1.711118   \n",
            "948  1095  ACGAATCTCTTACTCG.1   0.08631   257386.647319  0.0125 -1.987088   \n",
            "949  1556  AGCAACAAGGGTGAAC.1  0.890694    497204.83353  0.5875 -1.714032   \n",
            "950  2038  AGTACGCGTTTCCGGC.1  0.954596   661410.159517  0.5375 -1.531838   \n",
            "951  4684  CGGAATCGTTCCGGGA.1  0.732702   711655.272112    0.65 -1.134221   \n",
            "\n",
            "    heterotypic homotypic singlet  \n",
            "0          True     False   False  \n",
            "1         False     False    True  \n",
            "2         False      True   False  \n",
            "3          True     False   False  \n",
            "4         False     False    True  \n",
            "..          ...       ...     ...  \n",
            "947       False      True   False  \n",
            "948       False     False    True  \n",
            "949       False      True   False  \n",
            "950        True     False   False  \n",
            "951        True     False   False  \n",
            "\n",
            "[952 rows x 9 columns]\n",
            "Training data: <__main__.CustomDataloader object at 0x7f88f9e0e610>\n",
            "Validation data: <__main__.CustomDataloader object at 0x7f88f9e0e9d0>\n",
            "Testing data: <__main__.CustomDataloader object at 0x7f88f9e1dc50>\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "# import y and x\n",
        "import csv\n",
        "import pandas as pd\n",
        "dataset_read = pd.read_csv('Doubletannotation.csv')  \n",
        "\n",
        "# trimming the dataframe\n",
        "dataset_read = dataset_read[[\"V1\",\"bcds_s\",\"cxds_s\",\"dbf_s\",\"chord\",\"heterotypic\",\"homotypic\",\"singlet\"]]\n",
        "\n",
        "\n",
        "tempset = dataset_read\n",
        "train_split = tempset.sample(frac=0.8).reset_index()\n",
        "val_split = tempset.sample(frac=0.5).reset_index()\n",
        "test_split = tempset.sample(frac=1).reset_index()\n",
        "\n",
        "tt = train_split.to_numpy()\n",
        "t0 = np.where(tt[:,0+6] == True)\n",
        "t1 = np.where(tt[:,1+6] == True)\n",
        "t2 = np.where(tt[:,2+6] == True)\n",
        "t0set = tt[t0]\n",
        "t1set = tt[t1]\n",
        "t2set = tt[t2[0][:300]]\n",
        "print(t0set.shape)\n",
        "print(t1set.shape)\n",
        "print(t2set.shape)\n",
        "tset = np.concatenate((t0set, t1set), axis=0)\n",
        "tset = np.concatenate((tset, t2set), axis=0)\n",
        "np.random.shuffle(tset)\n",
        "print(tset)\n",
        "tset = pd.DataFrame(tset, columns=[\"idxs\",\"V1\",\"bcds_s\",\"cxds_s\",\"dbf_s\",\"chord\",\"heterotypic\",\"homotypic\",\"singlet\"])\n",
        "print(tset)\n",
        "\n",
        "\n",
        "train_data = CustomDataloader(tset)#train_split)\n",
        "val_data = CustomDataloader(val_split)\n",
        "test_data = CustomDataloader(test_split)\n",
        "\n",
        "loaders = {\n",
        "    'train' : train_data, #torch.utils.data.DataLoader(train_data, batch_size=100, shuffle=True, num_workers=1),\n",
        "    'val' : val_data, #torch.utils.data.DataLoader(val_data, batch_size=100, shuffle=True, num_workers=1),\n",
        "    'test'  : test_data #torch.utils.data.DataLoader(test_data, batch_size=100, shuffle=True, num_workers=1),\n",
        "}\n",
        "\n",
        "\n",
        "print(\"Training data:\" ,train_data)\n",
        "print(\"Validation data:\" ,val_data)\n",
        "print(\"Testing data:\" ,test_data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Data from h5ad"
      ],
      "metadata": {
        "id": "yfvskbXcFL6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "# import y and x\n",
        "import csv\n",
        "import pandas as pd\n",
        "dataset_read = pd.read_csv('pbmc_hvg_12012_100.csv')  \n",
        "\n",
        "clist = []\n",
        "\n",
        "\n",
        "for index, row in dataset_read.iterrows():\n",
        "  if row[-1] == \"singlet\":\n",
        "    clist.append([False, False, True])\n",
        "  if row[-1] == \"homo\":\n",
        "    clist.append([False, True, False])\n",
        "  if row[-1] == \"hetero\":\n",
        "    clist.append([True, False, False])\n",
        "\n",
        "classes = pd.DataFrame(data = np.array(clist), columns=[\"hetero\",\"homo\",\"singlet\"])\n",
        "print(sum(classes[\"hetero\"]))\n",
        "print(sum(classes[\"homo\"]))\n",
        "print(sum(classes[\"singlet\"]))\n",
        "# print(classes)\n",
        "\n",
        "print(dataset_read.shape)\n",
        "dataset_read = dataset_read.drop([\"Unnamed: 0\",'obs',\"cell_type\"], axis=1)\n",
        "print(dataset_read.shape)\n",
        "dataset_read = dataset_read.join(classes)\n",
        "print(dataset_read.shape)\n",
        "\n",
        "\n",
        "tempset = dataset_read\n",
        "train_split = tempset.sample(frac=0.8)\n",
        "tempset = tempset.drop(train_split.index)\n",
        "val_split = tempset.sample(frac=0.5)\n",
        "tempset = tempset.drop(val_split.index)\n",
        "test_split = tempset.sample(frac=1)\n",
        "# tempset = tempset.drop(test_split.index)\n",
        "\n",
        "train_split.reset_index()\n",
        "val_split.reset_index()\n",
        "test_split.reset_index()\n",
        "\n",
        "tt = train_split.to_numpy()\n",
        "t0 = np.where(tt[:,-3] == True)\n",
        "t1 = np.where(tt[:,-2] == True)\n",
        "t2 = np.where(tt[:,-1] == True)\n",
        "t0set = tt[t0]\n",
        "t1set = tt[t1]\n",
        "t2set = tt[t2[0][:400]]\n",
        "print(t0set.shape)\n",
        "print(t1set.shape)\n",
        "print(t2set.shape)\n",
        "tset = np.concatenate((t0set, t1set), axis=0)\n",
        "tset = np.concatenate((tset, t2set), axis=0)\n",
        "temp_var = tset\n",
        "# tset = tset[:,1:] # eliminate index column\n",
        "np.random.shuffle(tset)\n",
        "\n",
        "tset = pd.DataFrame(tset, columns=dataset_read.columns.values)#[1:])\n",
        "# print(tset.columns.values)\n",
        "\n",
        "# print(val_split.columns.values)\n",
        "# val_split = val_split.drop([\"index\"], axis=1)\n",
        "# test_split = test_split.drop([\"index\"], axis=1)\n",
        "\n",
        "print(tset.shape)\n",
        "\n",
        "train_data = CustomDataloader(tset)#train_split)\n",
        "val_data = CustomDataloader(val_split)\n",
        "test_data = CustomDataloader(test_split)\n",
        "\n",
        "loaders = {\n",
        "    'train' : train_data, #torch.utils.data.DataLoader(train_data, batch_size=100, shuffle=True, num_workers=1),\n",
        "    'val' : val_data, #torch.utils.data.DataLoader(val_data, batch_size=100, shuffle=True, num_workers=1),\n",
        "    'test'  : test_data #torch.utils.data.DataLoader(test_data, batch_size=100, shuffle=True, num_workers=1),\n",
        "}\n",
        "\n",
        "\n",
        "print(\"Training data:\" ,train_data)\n",
        "print(\"Validation data:\" ,val_data)\n",
        "print(\"Testing data:\" ,test_data)\n",
        "\n",
        "\n",
        "print(\"Training data:\" ,tset.shape)\n",
        "print(\"Validation data:\" ,val_split.shape)\n",
        "print(\"Testing data:\" ,test_split.shape)\n",
        "\n",
        "print(loaders[\"train\"][0][0].shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GC8fWQUnFPQZ",
        "outputId": "e180bdd6-cf4c-4973-b3ea-5f7aa1fbbde5"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "455\n",
            "361\n",
            "11196\n",
            "(12012, 103)\n",
            "(12012, 100)\n",
            "(12012, 103)\n",
            "(381, 103)\n",
            "(287, 103)\n",
            "(400, 103)\n",
            "(1068, 103)\n",
            "Training data: <__main__.CustomDataloader object at 0x7f0b722bcc10>\n",
            "Validation data: <__main__.CustomDataloader object at 0x7f0b722bc910>\n",
            "Testing data: <__main__.CustomDataloader object at 0x7f0b722bc090>\n",
            "Training data: (1068, 103)\n",
            "Validation data: (1201, 103)\n",
            "Testing data: (1201, 103)\n",
            "torch.Size([100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Architectur\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i1lClAZD7oh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "'''\n",
        "in_channels (int) — Number of channels in the input image\n",
        "\n",
        "out_channels (int) — Number of channels produced by the convolution\n",
        "\n",
        "kernel_size (int or tuple) — Size of the convolving kernel\n",
        "\n",
        "stride (int or tuple, optional) — Stride of the convolution. Default: 1\n",
        "\n",
        "padding (int or tuple, optional) — Zero-padding added to both sides of the input. Default: 0\n",
        "\n",
        "padding_mode (string, optional) — ‘zeros’, ‘reflect’, ‘replicate’ or ‘circular’. Default: ‘zeros’\n",
        "\n",
        "dilation (int or tuple, optional) — Spacing between kernel elements. Default: 1\n",
        "\n",
        "groups (int, optional) — Number of blocked connections from input channels to output channels. Default: 1\n",
        "\n",
        "bias (bool, optional) — If True, adds a learnable bias to the output. Default: True\n",
        "'''\n",
        "\n",
        "\n",
        "# https://medium.com/@nutanbhogendrasharma/pytorch-convolutional-neural-network-with-mnist-dataset-4e8a4265e118\n",
        "\n",
        "# hier gehts weiter\n",
        "\n",
        "import torch.nn as nn\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Sequential(         \n",
        "            nn.Conv2d(\n",
        "                in_channels=1,              \n",
        "                out_channels=16,            \n",
        "                kernel_size=5,              \n",
        "                stride=1,                   \n",
        "                padding=2,),                              \n",
        "            nn.ReLU(),                      \n",
        "            nn.MaxPool2d(kernel_size=2),)\n",
        "        self.conv2 = nn.Sequential(         \n",
        "            nn.Conv2d(16, 32, 5, 1, 2),     \n",
        "            nn.ReLU(),                      \n",
        "            nn.MaxPool2d(2),)\n",
        "        # fully connected layer, output 10 classes\n",
        "        self.out = nn.Linear(32 * 7 * 7, 10)\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
        "        x = x.view(x.size(0), -1)       \n",
        "        output = self.out(x)\n",
        "        return output#, x    # return x for visualization\n",
        "\n",
        "\n",
        "\n",
        "# Create ANN Model\n",
        "class ANNModel(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        # super(ANNModel, self).__init__()\n",
        "        super().__init__()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(input_size, 64) \n",
        "        self.fc2 = nn.Linear(64, 8)\n",
        "        self.fc3 = nn.Linear(8, 8)\n",
        "        self.fc4 = nn.Linear(8, 8)\n",
        "        self.fcend = nn.Linear(8, output_size)\n",
        "        self.sig = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # print(x.size())\n",
        "        # print(x)\n",
        "        # x = x.view(x.size(0), -1)       \n",
        "        out = self.fc1(x)\n",
        "        out = self.fc2(out)\n",
        "        out = self.fc3(out)\n",
        "        out = self.fc4(out)\n",
        "        out = self.fcend(out)\n",
        "        out = self.sig(out)\n",
        "        # out = self.fc1(x)\n",
        "        # out = self.fc2(out)\n",
        "        # out = self.fc3(out)\n",
        "        return out#.to(torch.float64)"
      ],
      "metadata": {
        "id": "qhVHZ3LC7nm5"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testt = torch.tensor([[[1,2,3],[1,2,3],[1,2,3],[1,2,3],[1,2,3],[1,2,3]],[[1,2,3],[1,2,3],[1,2,3],[1,2,3],[1,2,3],[1,2,3]]])\n",
        "print(testt)\n",
        "print(testt.size())\n",
        "print(testt.view(6,1))"
      ],
      "metadata": {
        "id": "0pFQ9Z1aVQRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Early Stopping"
      ],
      "metadata": {
        "id": "lyM7s7W48Zf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0.001, path='checkpoint.pt', trace_func=print):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pt'\n",
        "            trace_func (function): trace print function.\n",
        "                            Default: print            \n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "        self.best_model = 0\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Accuracy increased ({1-self.val_loss_min:.6f} --> {1-val_loss:.6f}).  Saving model ...')\n",
        "        self.best_model = model\n",
        "        #torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss"
      ],
      "metadata": {
        "id": "YbnZC1HPqRoi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training debugging"
      ],
      "metadata": {
        "id": "WjtU_WCB86_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn\n",
        "from torch import optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "NeuralNet = ANNModel(4,3)\n",
        "    # print(nnet)\n",
        "\n",
        "num_epochs = 100\n",
        "\n",
        "NeuralNet.train()\n",
        "\n",
        "# initialize the early_stopping object\n",
        "early_stopping = EarlyStopping(patience=5, verbose=True)\n",
        "    \n",
        "# loss\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "# optimizer\n",
        "optimizer = optim.Adam(NeuralNet.parameters(), lr = 0.01)   \n",
        "\n",
        "\n",
        "# Train the model\n",
        "total_step = len(loaders['train'])\n",
        "        \n",
        "for epoch in range(num_epochs):\n",
        "    NeuralNet.train()\n",
        "    print(epoch)\n",
        "    for images, labels in loaders['train']:\n",
        "        # gives batch data, normalize x when iterate train_loader\n",
        "        b_x = Variable(images)   # batch x\n",
        "        b_y = Variable(labels)   # batch y\n",
        "        \n",
        "        output = NeuralNet(b_x.float())\n",
        "                  \n",
        "        loss = loss_func(output, b_y)\n",
        "            \n",
        "        # clear gradients for this training step   \n",
        "        optimizer.zero_grad()           \n",
        "            \n",
        "        # backpropagation, compute gradients \n",
        "        loss.backward()    \n",
        "        # apply gradients             \n",
        "        optimizer.step()                \n",
        "        pass\n",
        "    NeuralNet.eval()\n",
        "    for data, labels in loaders['val']:\n",
        "      test_output = NeuralNet(data.float())\n",
        "      pred_y = torch.max(test_output,-1)[1]\n",
        "      true_y = torch.max(labels,-1)[1]\n",
        "      accuracy = int((pred_y == true_y).item())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "torch.max(test_output)\n",
        "test_output\n",
        "\n",
        "# outputs = [0,0,0]\n",
        "# print([int(i) for i in (test_output>=0.5).tolist()])\n",
        "# outputs[test_output >= 0.5] = 1\n",
        "# outputs[test_output < 0.5] = 0\n",
        "pred_y = torch.max(test_output,-1)[1]\n",
        "true_y = torch.max(labels,-1)[1]\n",
        "accuracy = int((pred_y == true_y).item())\n",
        "print(pred_y)\n",
        "print(true_y)\n",
        "print((pred_y == true_y).item())\n",
        "print(accuracy)"
      ],
      "metadata": {
        "id": "JxBEeFcffV9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NeuralNet = ANNModel(4,3)\n",
        "NeuralNet.train()\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(NeuralNet.parameters(), lr = 0.01)   \n",
        "\n",
        "point1, target1 = loaders[\"train\"][0:2]\n",
        "output = NeuralNet(point1.float())\n",
        "loss = loss_func(output, target1)\n",
        "# clear gradients for this training step   \n",
        "NeuralNet.zero_grad()   \n",
        "\n",
        "# print(loss)\n",
        "print('fc1.weight before backward')\n",
        "print(NeuralNet.fc1.weight[0])\n",
        "loss.backward()\n",
        "print('fc1.weight after backward')\n",
        "print(NeuralNet.fc1.weight[0])            \n",
        "optimizer.step()  \n",
        "print('fc1.weight after step')\n",
        "print(NeuralNet.fc1.weight[0])            \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKzfYXWXl7fl",
        "outputId": "5a6cf9b1-81ef-413f-9a0b-9d99ff52282b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fc1.weight before backward\n",
            "tensor([-0.3939,  0.0626, -0.4104, -0.1777], grad_fn=<SelectBackward0>)\n",
            "fc1.weight after backward\n",
            "tensor([-0.3939,  0.0626, -0.4104, -0.1777], grad_fn=<SelectBackward0>)\n",
            "fc1.weight after step\n",
            "tensor([-0.3939,  0.0626, -0.4104, -0.1777], grad_fn=<SelectBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "NeuralNet = ANNModel(4,3)\n",
        "NeuralNet.train()\n",
        "criterion = nn.MSELoss()\n",
        "print(loaders[\"train\"][0:2])\n",
        "unpack = loaders[\"train\"]\n",
        "print(unpack[:])\n",
        "point1, target1 = loaders[\"train\"][0:2]\n",
        "# create your optimizer\n",
        "optimizer = optim.SGD(NeuralNet.parameters(), lr=0.01)\n",
        "print(len(loaders[\"test\"]))\n",
        "\n",
        "# in your training loop:\n",
        "print('fc1.weight before backward')\n",
        "print(NeuralNet.fc1.weight[0])\n",
        "optimizer.zero_grad()   # zero the gradient buffers\n",
        "output = NeuralNet(point1)\n",
        "print(output)\n",
        "print(target1)\n",
        "loss = criterion(output, target1)\n",
        "test = NeuralNet.fc1.bias\n",
        "# print(\"gradient: \", NeuralNet.fc1.bias)\n",
        "loss.backward()\n",
        "optimizer.step()    # Does the update\n",
        "print('fc1.weight after step')\n",
        "print(NeuralNet.fc1.weight[0]) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 834
        },
        "id": "a2D6RJdfq9Kx",
        "outputId": "b8e6ad72-82e6-4d6d-974b-b1b909c8db91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[ 2.7279e-02,  6.8598e+04,  2.5000e-02, -1.9943e+00],\n",
            "        [ 3.1351e-02,  1.3335e+05,  1.1250e-01, -1.9835e+00]],\n",
            "       dtype=torch.float64), tensor([[0., 0., 1.],\n",
            "        [0., 0., 1.]], dtype=torch.float64))\n",
            "(tensor([[ 2.7279e-02,  6.8598e+04,  2.5000e-02, -1.9943e+00],\n",
            "        [ 3.1351e-02,  1.3335e+05,  1.1250e-01, -1.9835e+00],\n",
            "        [ 5.7130e-02,  2.1423e+05,  2.2500e-01, -1.9150e+00],\n",
            "        ...,\n",
            "        [ 4.5890e-02,  2.0553e+05,  0.0000e+00, -1.9908e+00],\n",
            "        [ 2.1363e-02,  2.7655e+05,  2.7500e-01, -1.8708e+00],\n",
            "        [ 4.5804e-02,  1.8010e+05,  3.7500e-02, -1.9908e+00]],\n",
            "       dtype=torch.float64), tensor([[0., 0., 1.],\n",
            "        [0., 0., 1.],\n",
            "        [0., 0., 1.],\n",
            "        ...,\n",
            "        [0., 0., 1.],\n",
            "        [0., 0., 1.],\n",
            "        [0., 0., 1.]], dtype=torch.float64))\n",
            "2402\n",
            "fc1.weight before backward\n",
            "tensor([-0.4072,  0.0744, -0.0804, -0.4936], grad_fn=<SelectBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-204-dc639016a48b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNeuralNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# zero the gradient buffers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoint1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-66465b3aefee>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# print(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Float but found Double"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = torch.tensor([[1,0],[0,1]])\n",
        "np.where(test[:1] == 1)\n",
        "#[where torch.tensor([1,0])[0] == 1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1YtaKgRx1bm",
        "outputId": "092a06fe-0d2f-491a-dc31-b3d16ac8ff73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0]), array([0]))"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label = torch.tensor([[0,0,1],[0,0,1],[0,0,1],[0,0,1],[0,0,1],[0,0,1],[0,0,1],[0,0,1],\n",
        "                     [0,1,0],[0,1,0],[0,1,0],[0,1,0],[0,1,0],[0,1,0],[0,1,0],[0,1,0],\n",
        "                     [1,0,0],[1,0,0],[1,0,0],[1,0,0],[1,0,0],[1,0,0],[1,0,0],[1,0,0]])\n",
        "\n",
        "prediction = torch.tensor([[1,0,0],[0,0,1],[0,0,1],[0,0,1],[0,0,1],[0,0,1],[0,0,1],[0,0,1],\n",
        "                          [0,0,1],[0,0,1],[0,1,0],[0,1,0],[0,1,0],[0,1,0],[0,1,0],[0,1,0],\n",
        "                          [0,1,0],[0,1,0],[0,1,0],[1,0,0],[1,0,0],[1,0,0],[1,0,0],[1,0,0]])\n",
        "\n",
        "indices_class0 = np.where(label[:0] == 1)\n",
        "label_class0 = label[indices_class0]\n",
        "prediction_class0 = prediction[indices_class0]\n",
        "pred_y0 = torch.max(prediction_class0,-1)[1]\n",
        "true_y0 = torch.max(label_class0,-1)[1]\n",
        "accuracy0 = int((pred_y0 == true_y0).item())\n",
        "\n",
        "indices_class1 = np.where(label[:1] == 1)\n",
        "label_class1 = label[indices_class1]\n",
        "prediction_class1 = prediction[indices_class1]\n",
        "pred_y1 = torch.max(prediction_class1,-1)[1]\n",
        "true_y1 = torch.max(label_class1,-1)[1]\n",
        "accuracy1 = int((pred_y1 == true_y1).item())\n",
        "\n",
        "indices_class2 = np.where(label[:2] == 1)\n",
        "label_class2 = label[indices_class2]\n",
        "prediction_class2 = prediction[indices_class2]\n",
        "pred_y2 = torch.max(prediction_class2,-1)[1]\n",
        "true_y2 = torch.max(label_class2,-1)[1]\n",
        "accuracy2 = int((pred_y2 == true_y2).item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "DQn6lVqzwUay",
        "outputId": "a3331c2a-73c2-4116-e202-9ca9d202484b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-26-8678919bc767>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    indices_class0 = [where label[0] == 1]\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "x1,y1 = loaders['train'][0]\n",
        "x2,y2 = loaders['train'][1]\n",
        "x3,y3 = loaders['train'][2]\n",
        "model = nn.Linear(4, 3)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.05)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# Run training\n",
        "niter = 10\n",
        "for _ in range(0, niter):\n",
        "\toptimizer.zero_grad()\n",
        "\tpredictions = model(x)\n",
        "\tloss = loss_fn(predictions, t)\n",
        "\tloss.backward()\n",
        "\toptimizer.step()\n",
        "\n",
        "\tprint(\"-\" * 10)\n",
        "\tprint(\"learned a = {}\".format(list(model.parameters())[0].data[0, 0]))\n",
        "\tprint(\"learned b = {}\".format(list(model.parameters())[1].data[0]))"
      ],
      "metadata": {
        "id": "O--Jue025Hxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "x1,y1 = loaders['train'][0:10]\n",
        "x2,y2 = loaders['train'][1]\n",
        "x3,y3 = loaders['train'][2]\n",
        "NeuralNet = ANNModel(4,3)\n",
        "# NeuralNet.train()\n",
        "optimizer = optim.Adam(NeuralNet.parameters(), lr = 0.01) \n",
        "criterion = nn.MSELoss()#weight=torch.tensor([1,1,0.02]))\n",
        "\n",
        "\n",
        "weight_before = NeuralNet.fc1.weight[0]\n",
        "# print(list(NeuralNet.parameters())[0].clone())\n",
        "optimizer.zero_grad()\n",
        "output = NeuralNet(x1)\n",
        "# print(output)\n",
        "loss = criterion(output, y1)\n",
        "# print(x1,y1)\n",
        "# print(loss.item())\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "weight_after = NeuralNet.fc1.weight[0]\n",
        "# print(weight_after-weight_before)\n",
        "# print(list(NeuralNet.parameters())[0].clone())\n",
        "\n",
        "print(y1)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "# Compute ROC curve and ROC area for each class\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(3):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y1[:, i].detach().numpy(), output[:, i].detach().numpy())\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Compute micro-average ROC curve and ROC area\n",
        "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y1.detach().numpy().ravel(), output.detach().numpy().ravel())\n",
        "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "lw = 2\n",
        "plt.plot(\n",
        "    fpr[2],\n",
        "    tpr[2],\n",
        "    color=\"darkorange\",\n",
        "    lw=lw,\n",
        "    label=\"ROC curve (area = %0.2f)\" % roc_auc[2],\n",
        ")\n",
        "plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"Receiver operating characteristic example\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        },
        "id": "d9tMg-q4wzz-",
        "outputId": "b1fcd4b5-a826-4318-973d-5d61f5defc16"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0.],\n",
            "        [0., 0., 1.],\n",
            "        [0., 1., 0.],\n",
            "        [1., 0., 0.],\n",
            "        [0., 0., 1.],\n",
            "        [1., 0., 0.],\n",
            "        [1., 0., 0.],\n",
            "        [0., 0., 1.],\n",
            "        [1., 0., 0.],\n",
            "        [0., 1., 0.]])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "jWDIU0oLfOMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# useful code \n",
        "def acc_for_class(let_class, labels, predictions):\n",
        "  indices_class = np.where(labels[:,let_class] == 1)\n",
        "  label_class = labels[indices_class]\n",
        "  prediction_class = predictions[indices_class]\n",
        "  # print(prediction_class.shape)\n",
        "  # print(label_class.shape)\n",
        "  pred_y = torch.max(prediction_class,-1)[1]\n",
        "  true_y = torch.max(label_class,-1)[1]\n",
        "  accuracy = sum(pred_y == true_y).item()/pred_y.__len__()\n",
        "  return accuracy\n",
        "\n",
        "def save_csv(data, name):\n",
        "  t_np = data.numpy() #convert to Numpy array\n",
        "  df = pd.DataFrame(t_np) #convert to a dataframe\n",
        "  df.to_csv(name+\".csv\",index=False) #save to file\n",
        "\n",
        "def to_strings(matrix):\n",
        "  strings = []\n",
        "  for i in matrix:\n",
        "    if i[0]:\n",
        "      strings.append(\"hetero\")\n",
        "    if i[1]:\n",
        "      strings.append(\"homo\")\n",
        "    if i[2]:\n",
        "      strings.append(\"singlet\")\n",
        "  return strings"
      ],
      "metadata": {
        "id": "6EMb872L2lKB"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "'''\n",
        "in_channels=1: because our input is a grayscale image.\n",
        "\n",
        "Stride: is the number of pixels to pass at a time when sliding the convolutional kernel.\n",
        "\n",
        "Padding: to preserve exactly the size of the input image, it is useful to add a zero padding on the border of the image.\n",
        "\n",
        "kernel_size: we need to define a kernel which is a small matrix of size 5 * 5. To perform the convolution operation, we just need to slide the kernel along the image horizontally and vertically and do the dot product of the kernel and the small portion of the image.\n",
        "\n",
        "The forward() pass defines the way we compute our output using the given layers and functions.\n",
        "'''\n",
        "import torch.nn\n",
        "from torch import optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "def train(num_epochs, NeuralNet, loaders):\n",
        "    \n",
        "    NeuralNet.train()\n",
        "\n",
        "    # initialize the early_stopping object\n",
        "    early_stopping = EarlyStopping(patience=10, verbose=True)\n",
        "    \n",
        "\n",
        "    total_step = len(loaders['train'])\n",
        "    _, labs = loaders['train'][:]\n",
        "    total_step\n",
        "    print(1-sum(labs[:,0])/total_step)\n",
        "    print(1-sum(labs[:,1])/total_step)\n",
        "    print(1-sum(labs[:,2])/total_step)\n",
        "\n",
        "    # optimizer\n",
        "    optimizer = optim.Adam(NeuralNet.parameters(), lr = 0.002)   \n",
        "\n",
        "    # loss\n",
        "    criterion = nn.MSELoss()#CrossEntropyLoss(weight=torch.tensor([1-sum(labs[:,0])/total_step, 1-sum(labs[:,1])/total_step, 1-sum(labs[:,2])/total_step]))\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in range(num_epochs):\n",
        "        NeuralNet.train()\n",
        "        print(epoch)\n",
        "        for images, labels in loaders['train']:\n",
        "            # clear gradients for this training step   \n",
        "            optimizer.zero_grad()\n",
        "            output = NeuralNet(images).to(torch.float64)\n",
        "            loss = criterion(output, labels.to(torch.float64))\n",
        "            # print(loss)\n",
        "            loss.backward()\n",
        "            optimizer.step()    # Does the update\n",
        "\n",
        "        # early_stopping needs the validation loss to check if it has decresed, and if it has, it will make a checkpoint of the current model\n",
        "        NeuralNet.eval()\n",
        "        test_output = torch.tensor([])\n",
        "        images, labels = loaders[\"val\"][:]\n",
        "        with torch.no_grad():\n",
        "          for imag, labe in loaders['val']:\n",
        "              test_out = NeuralNet(imag.to(torch.float32))\n",
        "\n",
        "              test_output = torch.cat((test_output,test_out.unsqueeze(0)),0)\n",
        "\n",
        "        acc0 = acc_for_class(0,labels, test_output)\n",
        "        acc1 = acc_for_class(1,labels, test_output)\n",
        "        acc2 = acc_for_class(2,labels, test_output)\n",
        "        early_stopping(1-(acc0+acc1+acc2)/3, NeuralNet)\n",
        "        print(\"accuracy mean: \", ((acc0+acc1+acc2)/3))\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            return early_stopping\n",
        "        pass\n",
        "    \n",
        "    \n",
        "    pass\n",
        "\n",
        "\n",
        "def test(model):\n",
        "    # Test the model\n",
        "    model.eval()\n",
        "    test_output = torch.tensor([])\n",
        "    images, labels = loaders[\"test\"][:]\n",
        "    with torch.no_grad():\n",
        "        for imag, labe in loaders['test']:\n",
        "            test_out = model(imag.to(torch.float32))\n",
        "\n",
        "            test_output = torch.cat((test_output,test_out.unsqueeze(0)),0)\n",
        "        \n",
        "        # save_csv(images,\"input\") # nicht wirklich gebraucht\n",
        "        save_csv(test_output,\"predictions\")\n",
        "        \n",
        "        \n",
        "        strings = to_strings(labels)\n",
        "        t_np = np.array(strings) #convert to Numpy array\n",
        "        df = pd.DataFrame(t_np) #convert to a dataframe\n",
        "        df.to_csv(\"labels.csv\",index=False) #save to file\n",
        "\n",
        "        acc0 = acc_for_class(0,labels, test_output)\n",
        "        acc1 = acc_for_class(1,labels, test_output)\n",
        "        acc2 = acc_for_class(2,labels, test_output)\n",
        "        print(\"Hetero acc: \",acc0)\n",
        "        print(\"Homo acc: \",acc1)\n",
        "        print(\"Sing acc: \",acc2)\n",
        "        \n",
        "        import matplotlib.pyplot as plt\n",
        "        from sklearn.metrics import roc_curve, auc\n",
        "        # Compute ROC curve and ROC area for each class\n",
        "        fpr = dict()\n",
        "        tpr = dict()\n",
        "        roc_auc = dict()\n",
        "        for i in range(3):\n",
        "            fpr[i], tpr[i], _ = roc_curve(labels[:, i].detach().numpy(), test_output[:, i].detach().numpy())\n",
        "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "        # Compute micro-average ROC curve and ROC area\n",
        "        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(labels.detach().numpy().ravel(), test_output.detach().numpy().ravel())\n",
        "        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "\n",
        "\n",
        "\n",
        "        plt.figure()\n",
        "        lw = 2\n",
        "        plt.plot(\n",
        "            fpr[2],\n",
        "            tpr[2],\n",
        "            color=\"darkorange\",\n",
        "            lw=lw,\n",
        "            label=\"ROC curve (area = %0.2f)\" % roc_auc[2],\n",
        "        )\n",
        "        plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel(\"False Positive Rate\")\n",
        "        plt.ylabel(\"True Positive Rate\")\n",
        "        plt.title(\"Receiver operating characteristic example\")\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    print('Test Accuracy of the model on test set: %.2f' % (1-(acc0+acc1+acc2)/3))\n",
        "    return (1-(acc0+acc1+acc2)/3)\n",
        "\n",
        "def acc_of_one_nn_run(NN):\n",
        "    nnet = NN\n",
        "    num_epochs = 100\n",
        "    ES = train(num_epochs, nnet, loaders)\n",
        "\n",
        "#    print(ES.best_model)\n",
        "\n",
        "\n",
        "    accuracy = test(ES.best_model)\n",
        "\n",
        "\n",
        "    # sample = next(iter(loaders['test']))\n",
        "    # imgs, lbls = sample\n",
        "\n",
        "    # actual_number = lbls[:10].numpy()\n",
        "    # actual_number\n",
        "\n",
        "    # test_output = ES.best_model(imgs[:10].float())\n",
        "    # pred_y = torch.max(test_output,-1)[1]\n",
        "    # print(f'Prediction number: {pred_y}')\n",
        "    # print(f'Actual number: {actual_number}')\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "acc_of_one_nn_run(ANNModel(100, 3))\n"
      ],
      "metadata": {
        "id": "Ybyxx1Nxn8Ki",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "65b96c55-032d-4c0a-d964-d6efab36dc6b"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.6433)\n",
            "tensor(0.7313)\n",
            "tensor(0.6255)\n",
            "0\n",
            "Accuracy increased (-inf --> 0.463008).  Saving model ...\n",
            "accuracy mean:  0.4630077668068906\n",
            "1\n",
            "Accuracy increased (0.463008 --> 0.520863).  Saving model ...\n",
            "accuracy mean:  0.520862849379164\n",
            "2\n",
            "Accuracy increased (0.520863 --> 0.547040).  Saving model ...\n",
            "accuracy mean:  0.5470404033799051\n",
            "3\n",
            "EarlyStopping counter: 1 out of 10\n",
            "accuracy mean:  0.525418310483373\n",
            "4\n",
            "EarlyStopping counter: 2 out of 10\n",
            "accuracy mean:  0.5250332396085355\n",
            "5\n",
            "EarlyStopping counter: 3 out of 10\n",
            "accuracy mean:  0.5398911629866969\n",
            "6\n",
            "EarlyStopping counter: 4 out of 10\n",
            "accuracy mean:  0.5232459295102335\n",
            "7\n",
            "EarlyStopping counter: 5 out of 10\n",
            "accuracy mean:  0.5473382883962888\n",
            "8\n",
            "EarlyStopping counter: 6 out of 10\n",
            "accuracy mean:  0.5117519271707462\n",
            "9\n",
            "EarlyStopping counter: 7 out of 10\n",
            "accuracy mean:  0.5289929306799771\n",
            "10\n",
            "EarlyStopping counter: 8 out of 10\n",
            "accuracy mean:  0.5051112709518516\n",
            "11\n",
            "EarlyStopping counter: 9 out of 10\n",
            "accuracy mean:  0.4834019921968657\n",
            "12\n",
            "EarlyStopping counter: 10 out of 10\n",
            "accuracy mean:  0.48319129303893577\n",
            "Early stopping\n",
            "Hetero acc:  0.7272727272727273\n",
            "Homo acc:  0.21212121212121213\n",
            "Sing acc:  0.6731277533039648\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy of the model on test set: 0.46\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4624927691006986"
            ]
          },
          "metadata": {},
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img, lbl = loaders[\"test\"][:]\n",
        "lbl = to_string(lbl)\n",
        "print(lbl.count(\"hetero\"))\n",
        "print(lbl.count(\"homo\"))\n",
        "print(lbl.count(\"singlet\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8qzHmB1ZhKT",
        "outputId": "1a0e3848-43d1-432d-a88d-f9e142389d25"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33\n",
            "33\n",
            "1135\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img, lbl = loaders[\"train\"][0]\n",
        "print(img.shape)\n",
        "testnet = ANNModel(102,3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEFV_LWzQiLb",
        "outputId": "1f222856-5cdd-435b-c2f9-e1d4b642201e"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([99])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "bs0BY4289PRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_of_one_nn_run(ANNModel, 4,3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "9XAzEnS3nwe5",
        "outputId": "836d24a6-154b-45c8-b855-f7d490ab54f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([2, 3])) that is different to the input size (torch.Size([3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-190-9587e4ac1f97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0macc_of_one_nn_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mANNModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-189-b9c86f99b23d>\u001b[0m in \u001b[0;36macc_of_one_nn_run\u001b[0;34m(NN, input_size, output_size)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0mES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;31m#    print(ES.best_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-189-b9c86f99b23d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_epochs, NeuralNet, loaders)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# Does the update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found dtype Double but expected Float"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create ANN\n",
        "ACCs = []\n",
        "ACCs.append(acc_of_one_nn_run(ANNModel, 4,3))\n",
        "ACCs.append(acc_of_one_nn_run(ANNModel, 4,3))\n",
        "ACCs.append(acc_of_one_nn_run(ANNModel, 4,3))\n",
        "ACCs.append(acc_of_one_nn_run(ANNModel, 4,3))\n",
        "ACCs.append(acc_of_one_nn_run(ANNModel, 4,3))\n",
        "\n",
        "print(\"Average accuracy of 5 ANNs: \", np.mean([i for i in ACCs]))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7vB1EEaJ2TSY",
        "outputId": "6653c289-22c5-41c1-da97-03fb2506ed68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mDie letzten 5000 Zeilen der Streamingausgabe wurden abgeschnitten.\u001b[0m\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n",
            "fine\n",
            "pre output\n",
            "forward done\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-316-55a0571b05ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create ANN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mACCs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mACCs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_of_one_nn_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mANNModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mACCs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_of_one_nn_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mANNModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mACCs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_of_one_nn_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mANNModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-315-127f1d775874>\u001b[0m in \u001b[0;36macc_of_one_nn_run\u001b[0;34m(NN, input_size, output_size)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0mES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;31m#    print(ES.best_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-315-127f1d775874>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_epochs, NeuralNet, loaders)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;31m# apply gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m  \u001b[0;31m#           if (i+1) % 100 == 0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    169\u001b[0m                  \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                  \u001b[0mforeach\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'foreach'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                  capturable=group['capturable'])\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    224\u001b[0m          \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m          \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m          capturable=capturable)\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create CNN\n",
        "ACCs_c = []\n",
        "ACCs_c.append(acc_of_one_nn_run(CNN))\n",
        "ACCs_c.append(acc_of_one_nn_run(CNN))\n",
        "ACCs_c.append(acc_of_one_nn_run(CNN))\n",
        "ACCs_c.append(acc_of_one_nn_run(CNN))\n",
        "ACCs_c.append(acc_of_one_nn_run(CNN))\n",
        "\n",
        "\n",
        "print(\"Average accuracy of 5 CNNs: \", np.mean([i for i in ACCs_c]))"
      ],
      "metadata": {
        "id": "qz_g07MRyfm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "count free parameters"
      ],
      "metadata": {
        "id": "4vmhgJOB9XXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "ANN_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(ANN_params)\n",
        "CNN_params = sum(p.numel() for p in CNN().parameters() if p.requires_grad)\n",
        "print(\"Average accuracy of 5 CNNs: \", CNN_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzaaxVGg5vZ3",
        "outputId": "ed30434c-d3fd-4c10-cdae-583fce0a7ecb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "648010\n",
            "28938\n"
          ]
        }
      ]
    }
  ]
}
